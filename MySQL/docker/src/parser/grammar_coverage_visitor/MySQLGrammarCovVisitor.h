#ifndef ANTLR_TEST_MySQLGrammarCovVisitor_H
#define ANTLR_TEST_MySQLGrammarCovVisitor_H

// DO NOT MODIFY THIS FILE. 
// This code is generated from PYTHON script generate_MySQLGrammarCovVisitor.h. 
// Use ANTLR4 to generate the MySQLParserBaseVisitor.h in ../grammar/ before calling the python generation script.

#include <iostream>
#include <cstring>
#include <filesystem>
#include <mutex>
#include <set>

#include "../MySQLBaseCommon.h"
#include "./grammar_cov_hash_header.h"
#include "../grammar/MySQLParserBaseVisitor.h"

using namespace std;
using namespace parsers;

//#define DEBUG
// Only for debugging purpose.
//#define LOGBLOCKCOV

#define LOGPATHCOV

class GramCovMap {

private:
  std::mutex edge_map_mutex;
  unsigned char *block_cov_map = nullptr;
  unsigned char *block_virgin_map = nullptr;
  unsigned char *edge_cov_map = nullptr;
  unsigned char *edge_virgin_map = nullptr;
  unsigned long long cur_path_hash = 0;
  set<unsigned long long> path_hash_set;

  unsigned int edge_prev_cov;

public:
  GramCovMap() {
    this->block_cov_map = new unsigned char[MAP_SIZE]();
    memset(this->block_cov_map, 0, MAP_SIZE);
    this->block_virgin_map = new unsigned char[MAP_SIZE]();
    memset(this->block_virgin_map, 0xff, MAP_SIZE);

    this->edge_cov_map = new unsigned char[MAP_SIZE]();
    memset(this->edge_cov_map, 0, MAP_SIZE);
    
    this->edge_virgin_map = new unsigned char[MAP_SIZE]();
    memset(this->edge_virgin_map, 0xff, MAP_SIZE);
    edge_prev_cov = 0;
  }
  ~GramCovMap() {
    delete[](this->block_cov_map);
    delete[](this->block_virgin_map);
    delete[](this->edge_cov_map);
    delete[](this->edge_virgin_map);
  }

  u8 has_new_grammar_bits(bool is_debug = false, const string in = "") {
#ifdef LOGBLOCKCOV
    // Only for debugging purpose.
    has_new_grammar_bits(this->block_cov_map, this->block_virgin_map, true, in);
#endif
#ifdef LOGPATHCOV
    has_new_path_hash(is_debug, in);
#endif
    return has_new_grammar_bits(this->edge_cov_map, this->edge_virgin_map, is_debug, in);
  }

  u8 has_new_path_hash(bool is_debug, const string in) {

    u8 ret = 0;
    edge_map_mutex.lock();
    if (this->path_hash_set.find(this->cur_path_hash) != this->path_hash_set.end()) {
      ret = 0;
    } else {
      path_hash_set.insert(this->cur_path_hash);
      ret = 1;
    }
    cur_path_hash = 0;
    edge_map_mutex.unlock();
    return ret;
  }
  
  u8 has_new_grammar_bits(u8 *cur_cov_map, u8 *cur_virgin_map,
                                    bool is_debug, const string in) {
  edge_map_mutex.lock();
  
#if defined(__x86_64__) || defined(__arm64__) || defined(__aarch64__)

      u64 *current = (u64 *)cur_cov_map;
      u64 *virgin = (u64 *)cur_virgin_map;
    
      u32 i = (MAP_SIZE >> 3);
    
    #else
    
      u32 *current = (u32 *)this->cov_map;
      u32 *virgin = (u32 *)this->virgin_map;
    
      u32 i = (MAP_SIZE >> 2);
    
    #endif /* ^__x86_64__ __arm64__ __aarch64__ */
    
      u8 ret = 0;
    
      while (i--) {
    
        /* Optimize for (*current & *virgin) == 0 - i.e., no bits in current bitmap
           that have not been already cleared from the virgin map - since this will
           almost always be the case. */
    
        if (unlikely(*current) && unlikely(*current & *virgin)) {
    
          if (likely(ret < 2) || unlikely(is_debug)) {
    
            u8 *cur = (u8 *)current;
            u8 *vir = (u8 *)virgin;
    
            /* Looks like we have not found any new bytes yet; see if any non-zero
               bytes in current[] are pristine in virgin[]. */
    
    #if defined(__x86_64__) || defined(__arm64__) || defined(__aarch64__)
    
            if ((cur[0] && vir[0] == 0xff) || (cur[1] && vir[1] == 0xff) ||
                (cur[2] && vir[2] == 0xff) || (cur[3] && vir[3] == 0xff) ||
                (cur[4] && vir[4] == 0xff) || (cur[5] && vir[5] == 0xff) ||
                (cur[6] && vir[6] == 0xff) || (cur[7] && vir[7] == 0xff)) {
              ret = 2;
              if (unlikely(is_debug)) {
                vector<u8> byte = get_cur_new_byte(cur, vir);
                for (const u8 &cur_byte : byte) {
                  this->gram_log_map_id(i, cur_byte, in);
                }
              }
            } else if (unlikely(ret != 2))
              ret = 1;
    
    #else
    
            if ((cur[0] && vir[0] == 0xff) || (cur[1] && vir[1] == 0xff) ||
                (cur[2] && vir[2] == 0xff) || (cur[3] && vir[3] == 0xff))
              ret = 2;
            else if (unlikely(ret != 2))
              ret = 1;
    
    #endif /* ^__x86_64__ __arm64__ __aarch64__ */
          }
          *virgin &= ~*current;
        }
    
        current++;
        virgin++;
      }
    
      edge_map_mutex.unlock();
      return ret;
  }

  void reset_block_cov_map() { memset(this->block_cov_map, 0, MAP_SIZE); }
  void reset_block_virgin_map() { memset(this->block_virgin_map, 0xff, MAP_SIZE); }

  void reset_edge_cov_map() {
    edge_map_mutex.lock();
    memset(this->edge_cov_map, 0, MAP_SIZE);
    edge_prev_cov = 0;
    edge_map_mutex.unlock();
  }
  void reset_edge_virgin_map() {
    edge_map_mutex.lock();
    memset(this->edge_virgin_map, 0xff, MAP_SIZE);
    edge_prev_cov = 0;
    edge_map_mutex.unlock();
  }

  void log_cov_map(unsigned int cur_cov) {
    edge_map_mutex.lock();
    unsigned int offset = (edge_prev_cov ^ cur_cov);
    if (edge_cov_map[offset] < 0xff) {
      edge_cov_map[offset]++;
    }
    edge_prev_cov = (cur_cov >> 1);
    edge_map_mutex.unlock();

    if (block_cov_map[cur_cov] < 0xff) {
      block_cov_map[cur_cov]++;
    }
  }

  void log_edge_cov_map(unsigned int prev_cov, unsigned int cur_cov) {
    edge_map_mutex.lock();
    unsigned int offset = ((prev_cov >> 1) ^ cur_cov);
    if (edge_cov_map[offset] < 0xff) {
      edge_cov_map[offset]++;
    }
#ifdef LOGBLOCKCOV
    if (block_cov_map[cur_cov] < 0xff) {
      block_cov_map[cur_cov]++;
    }
#endif
#ifdef LOGPATHCOV
    cur_path_hash = cur_path_hash ^ cur_cov;
#endif
    edge_map_mutex.unlock();
    return;
  }

  inline double get_total_block_cov_size() {
    u32 t_bytes = this->count_non_255_bytes(this->block_virgin_map);
    return ((double)t_bytes * 100.0) / MAP_SIZE;
  }
  inline u32 get_total_block_cov_size_num() {
    return this->count_non_255_bytes(this->block_virgin_map);
  }

  inline double get_total_edge_cov_size() {
    edge_map_mutex.lock();
    u32 t_bytes = this->count_non_255_bytes(this->edge_virgin_map);
    edge_map_mutex.unlock();
    return ((double)t_bytes * 100.0) / MAP_SIZE;
  }
  inline u32 get_total_edge_cov_size_num() {
    edge_map_mutex.lock();
    u32 res = this->count_non_255_bytes(this->edge_virgin_map);
    edge_map_mutex.unlock();
    return res;
  }
  inline u32 get_total_gramma_path_size_num() {
    edge_map_mutex.lock();
    u32 res = this->path_hash_set.size();
    edge_map_mutex.unlock();
    return res;
  }

  unsigned char *get_edge_cov_map() { return this->edge_cov_map; }

private:

  /* Count the number of non-255 bytes set in the bitmap. Used strictly for the
   status screen, several calls per second or so. */
  // Copy from afl-fuzz.
  u32 count_non_255_bytes(u8 *mem) {
#define FF(_b) (0xff << ((_b) << 3))
  u32 *ptr = (u32 *)mem;
  u32 i = (MAP_SIZE >> 2);
  u32 ret = 0;

  while (i--) {

    u32 v = *(ptr++);

    /* This is called on the virgin bitmap, so optimize for the most likely
       case. */

    if (v == 0xffffffff)
      continue;
    if ((v & FF(0)) != FF(0))
      ret++;
    if ((v & FF(1)) != FF(1))
      ret++;
    if ((v & FF(2)) != FF(2))
      ret++;
    if ((v & FF(3)) != FF(3))
      ret++;
  }

  return ret;
#undef FF
  }

  inline vector<u8> get_cur_new_byte(u8 *cur, u8 *vir) {
    vector<u8> new_byte_v;
    for (u8 i = 0; i < 8; i++) {
      if (cur[i] && vir[i] == 0xff)
        new_byte_v.push_back(i);
    }
    return new_byte_v;
  }

  inline void gram_log_map_id (u32 i, u8 byte, const string in = "") {
    fstream gram_id_out;
    i = (MAP_SIZE >> 3) - i - 1 ;
    u32 actual_idx = i * 8 + byte;

    if (!filesystem::exists("./gram_cov.txt")) {
      gram_id_out.open("./gram_cov.txt", std::fstream::out |
      std::fstream::trunc);
    } else {
      gram_id_out.open("./gram_cov.txt", std::fstream::out |
      std::fstream::app);
    }
    gram_id_out << actual_idx << endl;
    gram_id_out.flush();
    gram_id_out.close();

    if (!filesystem::exists("./new_gram_file/")) {
      filesystem::create_directory("./new_gram_file/");
    }
    fstream map_id_seed_output;
    map_id_seed_output.open(
        "./new_gram_file/" + to_string(actual_idx) + ".txt",
        std::fstream::out | std::fstream::trunc);
    map_id_seed_output << in;
    map_id_seed_output.close();

  }
};

class MySQLGrammarCovVisitor: public parsers::MySQLParserBaseVisitor {
private:
  // A randomly generated beforehand but runtime fixed Hash Array.
  HASHARRAYDEFINE;
  
  MySQLParser* p_parser;

public:

  void set_parser(MySQLParser* in) {this->p_parser = in;}
  
  GramCovMap gram_cov;

  virtual std::any visitQuery(MySQLParser::QueryContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSimpleStatement(MySQLParser::SimpleStatementContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitAlterStatement(MySQLParser::AlterStatementContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitAlterDatabase(MySQLParser::AlterDatabaseContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitAlterEvent(MySQLParser::AlterEventContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitAlterLogfileGroup(MySQLParser::AlterLogfileGroupContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitAlterLogfileGroupOptions(MySQLParser::AlterLogfileGroupOptionsContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitAlterLogfileGroupOption(MySQLParser::AlterLogfileGroupOptionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitAlterServer(MySQLParser::AlterServerContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitAlterTable(MySQLParser::AlterTableContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitAlterTableActions(MySQLParser::AlterTableActionsContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitAlterCommandList(MySQLParser::AlterCommandListContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitAlterCommandsModifierList(MySQLParser::AlterCommandsModifierListContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitStandaloneAlterCommands(MySQLParser::StandaloneAlterCommandsContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitAlterPartition(MySQLParser::AlterPartitionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitAlterList(MySQLParser::AlterListContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitAlterCommandsModifier(MySQLParser::AlterCommandsModifierContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitAlterListItem(MySQLParser::AlterListItemContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitPlace(MySQLParser::PlaceContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitRestrict(MySQLParser::RestrictContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitAlterOrderList(MySQLParser::AlterOrderListContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitAlterAlgorithmOption(MySQLParser::AlterAlgorithmOptionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitAlterLockOption(MySQLParser::AlterLockOptionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitIndexLockAndAlgorithm(MySQLParser::IndexLockAndAlgorithmContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitWithValidation(MySQLParser::WithValidationContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitRemovePartitioning(MySQLParser::RemovePartitioningContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitAllOrPartitionNameList(MySQLParser::AllOrPartitionNameListContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitAlterTablespace(MySQLParser::AlterTablespaceContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitAlterUndoTablespace(MySQLParser::AlterUndoTablespaceContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitUndoTableSpaceOptions(MySQLParser::UndoTableSpaceOptionsContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitUndoTableSpaceOption(MySQLParser::UndoTableSpaceOptionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitAlterTablespaceOptions(MySQLParser::AlterTablespaceOptionsContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitAlterTablespaceOption(MySQLParser::AlterTablespaceOptionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitChangeTablespaceOption(MySQLParser::ChangeTablespaceOptionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitAlterView(MySQLParser::AlterViewContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitViewTail(MySQLParser::ViewTailContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitViewSelect(MySQLParser::ViewSelectContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitViewCheckOption(MySQLParser::ViewCheckOptionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitCreateStatement(MySQLParser::CreateStatementContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitCreateDatabase(MySQLParser::CreateDatabaseContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitCreateDatabaseOption(MySQLParser::CreateDatabaseOptionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitCreateTable(MySQLParser::CreateTableContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitTableElementList(MySQLParser::TableElementListContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitTableElement(MySQLParser::TableElementContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitDuplicateAsQueryExpression(MySQLParser::DuplicateAsQueryExpressionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitQueryExpressionOrParens(MySQLParser::QueryExpressionOrParensContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitCreateRoutine(MySQLParser::CreateRoutineContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitCreateProcedure(MySQLParser::CreateProcedureContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitCreateFunction(MySQLParser::CreateFunctionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitCreateUdf(MySQLParser::CreateUdfContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitRoutineCreateOption(MySQLParser::RoutineCreateOptionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitRoutineAlterOptions(MySQLParser::RoutineAlterOptionsContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitRoutineOption(MySQLParser::RoutineOptionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitCreateIndex(MySQLParser::CreateIndexContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitIndexNameAndType(MySQLParser::IndexNameAndTypeContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitCreateIndexTarget(MySQLParser::CreateIndexTargetContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitCreateLogfileGroup(MySQLParser::CreateLogfileGroupContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitLogfileGroupOptions(MySQLParser::LogfileGroupOptionsContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitLogfileGroupOption(MySQLParser::LogfileGroupOptionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitCreateServer(MySQLParser::CreateServerContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitServerOptions(MySQLParser::ServerOptionsContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitServerOption(MySQLParser::ServerOptionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitCreateTablespace(MySQLParser::CreateTablespaceContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitCreateUndoTablespace(MySQLParser::CreateUndoTablespaceContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitTsDataFileName(MySQLParser::TsDataFileNameContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitTsDataFile(MySQLParser::TsDataFileContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitTablespaceOptions(MySQLParser::TablespaceOptionsContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitTablespaceOption(MySQLParser::TablespaceOptionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitTsOptionInitialSize(MySQLParser::TsOptionInitialSizeContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitTsOptionUndoRedoBufferSize(MySQLParser::TsOptionUndoRedoBufferSizeContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitTsOptionAutoextendSize(MySQLParser::TsOptionAutoextendSizeContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitTsOptionMaxSize(MySQLParser::TsOptionMaxSizeContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitTsOptionExtentSize(MySQLParser::TsOptionExtentSizeContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitTsOptionNodegroup(MySQLParser::TsOptionNodegroupContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitTsOptionEngine(MySQLParser::TsOptionEngineContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitTsOptionWait(MySQLParser::TsOptionWaitContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitTsOptionComment(MySQLParser::TsOptionCommentContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitTsOptionFileblockSize(MySQLParser::TsOptionFileblockSizeContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitTsOptionEncryption(MySQLParser::TsOptionEncryptionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitCreateView(MySQLParser::CreateViewContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitViewReplaceOrAlgorithm(MySQLParser::ViewReplaceOrAlgorithmContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitViewAlgorithm(MySQLParser::ViewAlgorithmContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitViewSuid(MySQLParser::ViewSuidContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitCreateTrigger(MySQLParser::CreateTriggerContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitTriggerFollowsPrecedesClause(MySQLParser::TriggerFollowsPrecedesClauseContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitCreateEvent(MySQLParser::CreateEventContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitCreateRole(MySQLParser::CreateRoleContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitCreateSpatialReference(MySQLParser::CreateSpatialReferenceContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSrsAttribute(MySQLParser::SrsAttributeContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitDropStatement(MySQLParser::DropStatementContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitDropDatabase(MySQLParser::DropDatabaseContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitDropEvent(MySQLParser::DropEventContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitDropFunction(MySQLParser::DropFunctionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitDropProcedure(MySQLParser::DropProcedureContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitDropIndex(MySQLParser::DropIndexContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitDropLogfileGroup(MySQLParser::DropLogfileGroupContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitDropLogfileGroupOption(MySQLParser::DropLogfileGroupOptionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitDropServer(MySQLParser::DropServerContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitDropTable(MySQLParser::DropTableContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitDropTableSpace(MySQLParser::DropTableSpaceContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitDropTrigger(MySQLParser::DropTriggerContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitDropView(MySQLParser::DropViewContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitDropRole(MySQLParser::DropRoleContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitDropSpatialReference(MySQLParser::DropSpatialReferenceContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitDropUndoTablespace(MySQLParser::DropUndoTablespaceContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitRenameTableStatement(MySQLParser::RenameTableStatementContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitRenamePair(MySQLParser::RenamePairContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitTruncateTableStatement(MySQLParser::TruncateTableStatementContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitImportStatement(MySQLParser::ImportStatementContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitCallStatement(MySQLParser::CallStatementContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitDeleteStatement(MySQLParser::DeleteStatementContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitPartitionDelete(MySQLParser::PartitionDeleteContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitDeleteStatementOption(MySQLParser::DeleteStatementOptionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitDoStatement(MySQLParser::DoStatementContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitHandlerStatement(MySQLParser::HandlerStatementContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitHandlerReadOrScan(MySQLParser::HandlerReadOrScanContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitInsertStatement(MySQLParser::InsertStatementContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitInsertLockOption(MySQLParser::InsertLockOptionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitInsertFromConstructor(MySQLParser::InsertFromConstructorContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitFields(MySQLParser::FieldsContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitInsertValues(MySQLParser::InsertValuesContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitInsertQueryExpression(MySQLParser::InsertQueryExpressionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitValueList(MySQLParser::ValueListContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitValues(MySQLParser::ValuesContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitValuesReference(MySQLParser::ValuesReferenceContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitInsertUpdateList(MySQLParser::InsertUpdateListContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitLoadStatement(MySQLParser::LoadStatementContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitDataOrXml(MySQLParser::DataOrXmlContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitXmlRowsIdentifiedBy(MySQLParser::XmlRowsIdentifiedByContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitLoadDataFileTail(MySQLParser::LoadDataFileTailContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitLoadDataFileTargetList(MySQLParser::LoadDataFileTargetListContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitFieldOrVariableList(MySQLParser::FieldOrVariableListContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitReplaceStatement(MySQLParser::ReplaceStatementContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSelectStatement(MySQLParser::SelectStatementContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSelectStatementWithInto(MySQLParser::SelectStatementWithIntoContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitQueryExpression(MySQLParser::QueryExpressionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitQueryExpressionBody(MySQLParser::QueryExpressionBodyContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitQueryExpressionParens(MySQLParser::QueryExpressionParensContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitQueryPrimary(MySQLParser::QueryPrimaryContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitQuerySpecification(MySQLParser::QuerySpecificationContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSubquery(MySQLParser::SubqueryContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitQuerySpecOption(MySQLParser::QuerySpecOptionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitLimitClause(MySQLParser::LimitClauseContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSimpleLimitClause(MySQLParser::SimpleLimitClauseContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitLimitOptions(MySQLParser::LimitOptionsContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitLimitOption(MySQLParser::LimitOptionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitIntoClause(MySQLParser::IntoClauseContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitProcedureAnalyseClause(MySQLParser::ProcedureAnalyseClauseContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitHavingClause(MySQLParser::HavingClauseContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitWindowClause(MySQLParser::WindowClauseContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitWindowDefinition(MySQLParser::WindowDefinitionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitWindowSpec(MySQLParser::WindowSpecContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitWindowSpecDetails(MySQLParser::WindowSpecDetailsContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitWindowFrameClause(MySQLParser::WindowFrameClauseContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitWindowFrameUnits(MySQLParser::WindowFrameUnitsContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitWindowFrameExtent(MySQLParser::WindowFrameExtentContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitWindowFrameStart(MySQLParser::WindowFrameStartContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitWindowFrameBetween(MySQLParser::WindowFrameBetweenContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitWindowFrameBound(MySQLParser::WindowFrameBoundContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitWindowFrameExclusion(MySQLParser::WindowFrameExclusionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitWithClause(MySQLParser::WithClauseContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitCommonTableExpression(MySQLParser::CommonTableExpressionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitGroupByClause(MySQLParser::GroupByClauseContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitOlapOption(MySQLParser::OlapOptionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitOrderClause(MySQLParser::OrderClauseContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitDirection(MySQLParser::DirectionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitFromClause(MySQLParser::FromClauseContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitTableReferenceList(MySQLParser::TableReferenceListContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitTableValueConstructor(MySQLParser::TableValueConstructorContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitExplicitTable(MySQLParser::ExplicitTableContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitRowValueExplicit(MySQLParser::RowValueExplicitContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSelectOption(MySQLParser::SelectOptionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitLockingClauseList(MySQLParser::LockingClauseListContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitLockingClause(MySQLParser::LockingClauseContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitLockStrengh(MySQLParser::LockStrenghContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitLockedRowAction(MySQLParser::LockedRowActionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSelectItemList(MySQLParser::SelectItemListContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSelectItem(MySQLParser::SelectItemContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSelectAlias(MySQLParser::SelectAliasContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitWhereClause(MySQLParser::WhereClauseContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitTableReference(MySQLParser::TableReferenceContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitEscapedTableReference(MySQLParser::EscapedTableReferenceContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitJoinedTable(MySQLParser::JoinedTableContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitNaturalJoinType(MySQLParser::NaturalJoinTypeContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitInnerJoinType(MySQLParser::InnerJoinTypeContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitOuterJoinType(MySQLParser::OuterJoinTypeContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitTableFactor(MySQLParser::TableFactorContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSingleTable(MySQLParser::SingleTableContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSingleTableParens(MySQLParser::SingleTableParensContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitDerivedTable(MySQLParser::DerivedTableContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitTableReferenceListParens(MySQLParser::TableReferenceListParensContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitTableFunction(MySQLParser::TableFunctionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitColumnsClause(MySQLParser::ColumnsClauseContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitJtColumn(MySQLParser::JtColumnContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitOnEmptyOrError(MySQLParser::OnEmptyOrErrorContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitOnEmpty(MySQLParser::OnEmptyContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitOnError(MySQLParser::OnErrorContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitJtOnResponse(MySQLParser::JtOnResponseContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitUnionOption(MySQLParser::UnionOptionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitTableAlias(MySQLParser::TableAliasContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitIndexHintList(MySQLParser::IndexHintListContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitIndexHint(MySQLParser::IndexHintContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitIndexHintType(MySQLParser::IndexHintTypeContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitKeyOrIndex(MySQLParser::KeyOrIndexContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitConstraintKeyType(MySQLParser::ConstraintKeyTypeContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitIndexHintClause(MySQLParser::IndexHintClauseContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitIndexList(MySQLParser::IndexListContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitIndexListElement(MySQLParser::IndexListElementContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitUpdateStatement(MySQLParser::UpdateStatementContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitTransactionOrLockingStatement(MySQLParser::TransactionOrLockingStatementContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitTransactionStatement(MySQLParser::TransactionStatementContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitBeginWork(MySQLParser::BeginWorkContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitTransactionCharacteristic(MySQLParser::TransactionCharacteristicContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSavepointStatement(MySQLParser::SavepointStatementContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitLockStatement(MySQLParser::LockStatementContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitLockItem(MySQLParser::LockItemContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitLockOption(MySQLParser::LockOptionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitXaStatement(MySQLParser::XaStatementContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitXaConvert(MySQLParser::XaConvertContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitXid(MySQLParser::XidContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitReplicationStatement(MySQLParser::ReplicationStatementContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitResetOption(MySQLParser::ResetOptionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitMasterResetOptions(MySQLParser::MasterResetOptionsContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitReplicationLoad(MySQLParser::ReplicationLoadContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitChangeMaster(MySQLParser::ChangeMasterContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitChangeMasterOptions(MySQLParser::ChangeMasterOptionsContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitMasterOption(MySQLParser::MasterOptionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitPrivilegeCheckDef(MySQLParser::PrivilegeCheckDefContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitTablePrimaryKeyCheckDef(MySQLParser::TablePrimaryKeyCheckDefContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitMasterTlsCiphersuitesDef(MySQLParser::MasterTlsCiphersuitesDefContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitMasterFileDef(MySQLParser::MasterFileDefContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitServerIdList(MySQLParser::ServerIdListContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitChangeReplication(MySQLParser::ChangeReplicationContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitFilterDefinition(MySQLParser::FilterDefinitionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitFilterDbList(MySQLParser::FilterDbListContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitFilterTableList(MySQLParser::FilterTableListContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitFilterStringList(MySQLParser::FilterStringListContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitFilterWildDbTableString(MySQLParser::FilterWildDbTableStringContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitFilterDbPairList(MySQLParser::FilterDbPairListContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSlave(MySQLParser::SlaveContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSlaveUntilOptions(MySQLParser::SlaveUntilOptionsContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSlaveConnectionOptions(MySQLParser::SlaveConnectionOptionsContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSlaveThreadOptions(MySQLParser::SlaveThreadOptionsContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSlaveThreadOption(MySQLParser::SlaveThreadOptionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitGroupReplication(MySQLParser::GroupReplicationContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitPreparedStatement(MySQLParser::PreparedStatementContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitExecuteStatement(MySQLParser::ExecuteStatementContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitExecuteVarList(MySQLParser::ExecuteVarListContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitCloneStatement(MySQLParser::CloneStatementContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitDataDirSSL(MySQLParser::DataDirSSLContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSsl(MySQLParser::SslContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitAccountManagementStatement(MySQLParser::AccountManagementStatementContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitAlterUser(MySQLParser::AlterUserContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitAlterUserTail(MySQLParser::AlterUserTailContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitUserFunction(MySQLParser::UserFunctionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitCreateUser(MySQLParser::CreateUserContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitCreateUserTail(MySQLParser::CreateUserTailContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitDefaultRoleClause(MySQLParser::DefaultRoleClauseContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitRequireClause(MySQLParser::RequireClauseContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitConnectOptions(MySQLParser::ConnectOptionsContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitAccountLockPasswordExpireOptions(MySQLParser::AccountLockPasswordExpireOptionsContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitDropUser(MySQLParser::DropUserContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitGrant(MySQLParser::GrantContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitGrantTargetList(MySQLParser::GrantTargetListContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitGrantOptions(MySQLParser::GrantOptionsContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitExceptRoleList(MySQLParser::ExceptRoleListContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitWithRoles(MySQLParser::WithRolesContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitGrantAs(MySQLParser::GrantAsContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitVersionedRequireClause(MySQLParser::VersionedRequireClauseContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitRenameUser(MySQLParser::RenameUserContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitRevoke(MySQLParser::RevokeContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitOnTypeTo(MySQLParser::OnTypeToContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitAclType(MySQLParser::AclTypeContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitRoleOrPrivilegesList(MySQLParser::RoleOrPrivilegesListContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitRoleOrPrivilege(MySQLParser::RoleOrPrivilegeContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitGrantIdentifier(MySQLParser::GrantIdentifierContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitRequireList(MySQLParser::RequireListContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitRequireListElement(MySQLParser::RequireListElementContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitGrantOption(MySQLParser::GrantOptionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSetRole(MySQLParser::SetRoleContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitRoleList(MySQLParser::RoleListContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitRole(MySQLParser::RoleContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitTableAdministrationStatement(MySQLParser::TableAdministrationStatementContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitHistogram(MySQLParser::HistogramContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitCheckOption(MySQLParser::CheckOptionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitRepairType(MySQLParser::RepairTypeContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitInstallUninstallStatment(MySQLParser::InstallUninstallStatmentContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSetStatement(MySQLParser::SetStatementContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitStartOptionValueList(MySQLParser::StartOptionValueListContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitTransactionCharacteristics(MySQLParser::TransactionCharacteristicsContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitTransactionAccessMode(MySQLParser::TransactionAccessModeContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitIsolationLevel(MySQLParser::IsolationLevelContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitOptionValueListContinued(MySQLParser::OptionValueListContinuedContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitOptionValueNoOptionType(MySQLParser::OptionValueNoOptionTypeContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitOptionValue(MySQLParser::OptionValueContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSetSystemVariable(MySQLParser::SetSystemVariableContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitStartOptionValueListFollowingOptionType(MySQLParser::StartOptionValueListFollowingOptionTypeContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitOptionValueFollowingOptionType(MySQLParser::OptionValueFollowingOptionTypeContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSetExprOrDefault(MySQLParser::SetExprOrDefaultContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitShowStatement(MySQLParser::ShowStatementContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitShowCommandType(MySQLParser::ShowCommandTypeContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitNonBlocking(MySQLParser::NonBlockingContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitFromOrIn(MySQLParser::FromOrInContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitInDb(MySQLParser::InDbContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitProfileType(MySQLParser::ProfileTypeContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitOtherAdministrativeStatement(MySQLParser::OtherAdministrativeStatementContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitKeyCacheListOrParts(MySQLParser::KeyCacheListOrPartsContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitKeyCacheList(MySQLParser::KeyCacheListContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitAssignToKeycache(MySQLParser::AssignToKeycacheContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitAssignToKeycachePartition(MySQLParser::AssignToKeycachePartitionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitCacheKeyList(MySQLParser::CacheKeyListContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitKeyUsageElement(MySQLParser::KeyUsageElementContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitKeyUsageList(MySQLParser::KeyUsageListContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitFlushOption(MySQLParser::FlushOptionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitLogType(MySQLParser::LogTypeContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitFlushTables(MySQLParser::FlushTablesContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitFlushTablesOptions(MySQLParser::FlushTablesOptionsContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitPreloadTail(MySQLParser::PreloadTailContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitPreloadList(MySQLParser::PreloadListContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitPreloadKeys(MySQLParser::PreloadKeysContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitAdminPartition(MySQLParser::AdminPartitionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitResourceGroupManagement(MySQLParser::ResourceGroupManagementContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitCreateResourceGroup(MySQLParser::CreateResourceGroupContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitResourceGroupVcpuList(MySQLParser::ResourceGroupVcpuListContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitVcpuNumOrRange(MySQLParser::VcpuNumOrRangeContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitResourceGroupPriority(MySQLParser::ResourceGroupPriorityContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitResourceGroupEnableDisable(MySQLParser::ResourceGroupEnableDisableContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitAlterResourceGroup(MySQLParser::AlterResourceGroupContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSetResourceGroup(MySQLParser::SetResourceGroupContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitThreadIdList(MySQLParser::ThreadIdListContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitDropResourceGroup(MySQLParser::DropResourceGroupContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitUtilityStatement(MySQLParser::UtilityStatementContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitDescribeStatement(MySQLParser::DescribeStatementContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitExplainStatement(MySQLParser::ExplainStatementContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitExplainableStatement(MySQLParser::ExplainableStatementContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitHelpCommand(MySQLParser::HelpCommandContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitUseCommand(MySQLParser::UseCommandContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitRestartServer(MySQLParser::RestartServerContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitExprOr(MySQLParser::ExprOrContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitExprNot(MySQLParser::ExprNotContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitExprIs(MySQLParser::ExprIsContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitExprAnd(MySQLParser::ExprAndContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitExprXor(MySQLParser::ExprXorContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitPrimaryExprPredicate(MySQLParser::PrimaryExprPredicateContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitPrimaryExprCompare(MySQLParser::PrimaryExprCompareContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitPrimaryExprAllAny(MySQLParser::PrimaryExprAllAnyContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitPrimaryExprIsNull(MySQLParser::PrimaryExprIsNullContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitCompOp(MySQLParser::CompOpContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitPredicate(MySQLParser::PredicateContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitPredicateExprIn(MySQLParser::PredicateExprInContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitPredicateExprBetween(MySQLParser::PredicateExprBetweenContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitPredicateExprLike(MySQLParser::PredicateExprLikeContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitPredicateExprRegex(MySQLParser::PredicateExprRegexContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitBitExpr(MySQLParser::BitExprContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSimpleExprConvert(MySQLParser::SimpleExprConvertContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSimpleExprVariable(MySQLParser::SimpleExprVariableContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSimpleExprCast(MySQLParser::SimpleExprCastContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSimpleExprUnary(MySQLParser::SimpleExprUnaryContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSimpleExprOdbc(MySQLParser::SimpleExprOdbcContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSimpleExprRuntimeFunction(MySQLParser::SimpleExprRuntimeFunctionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSimpleExprFunction(MySQLParser::SimpleExprFunctionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSimpleExprCollate(MySQLParser::SimpleExprCollateContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSimpleExprMatch(MySQLParser::SimpleExprMatchContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSimpleExprWindowingFunction(MySQLParser::SimpleExprWindowingFunctionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSimpleExprBinary(MySQLParser::SimpleExprBinaryContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSimpleExprColumnRef(MySQLParser::SimpleExprColumnRefContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSimpleExprParamMarker(MySQLParser::SimpleExprParamMarkerContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSimpleExprSum(MySQLParser::SimpleExprSumContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSimpleExprConvertUsing(MySQLParser::SimpleExprConvertUsingContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSimpleExprSubQuery(MySQLParser::SimpleExprSubQueryContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSimpleExprGroupingOperation(MySQLParser::SimpleExprGroupingOperationContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSimpleExprNot(MySQLParser::SimpleExprNotContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSimpleExprValues(MySQLParser::SimpleExprValuesContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSimpleExprDefault(MySQLParser::SimpleExprDefaultContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSimpleExprList(MySQLParser::SimpleExprListContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSimpleExprInterval(MySQLParser::SimpleExprIntervalContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSimpleExprCase(MySQLParser::SimpleExprCaseContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSimpleExprConcat(MySQLParser::SimpleExprConcatContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSimpleExprLiteral(MySQLParser::SimpleExprLiteralContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitArrayCast(MySQLParser::ArrayCastContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitJsonOperator(MySQLParser::JsonOperatorContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSumExpr(MySQLParser::SumExprContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitGroupingOperation(MySQLParser::GroupingOperationContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitWindowFunctionCall(MySQLParser::WindowFunctionCallContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitWindowingClause(MySQLParser::WindowingClauseContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitLeadLagInfo(MySQLParser::LeadLagInfoContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitNullTreatment(MySQLParser::NullTreatmentContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitJsonFunction(MySQLParser::JsonFunctionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitInSumExpr(MySQLParser::InSumExprContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitIdentListArg(MySQLParser::IdentListArgContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitIdentList(MySQLParser::IdentListContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitFulltextOptions(MySQLParser::FulltextOptionsContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitRuntimeFunctionCall(MySQLParser::RuntimeFunctionCallContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitGeometryFunction(MySQLParser::GeometryFunctionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitTimeFunctionParameters(MySQLParser::TimeFunctionParametersContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitFractionalPrecision(MySQLParser::FractionalPrecisionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitWeightStringLevels(MySQLParser::WeightStringLevelsContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitWeightStringLevelListItem(MySQLParser::WeightStringLevelListItemContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitDateTimeTtype(MySQLParser::DateTimeTtypeContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitTrimFunction(MySQLParser::TrimFunctionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSubstringFunction(MySQLParser::SubstringFunctionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitFunctionCall(MySQLParser::FunctionCallContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitUdfExprList(MySQLParser::UdfExprListContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitUdfExpr(MySQLParser::UdfExprContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitVariable(MySQLParser::VariableContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitUserVariable(MySQLParser::UserVariableContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSystemVariable(MySQLParser::SystemVariableContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitInternalVariableName(MySQLParser::InternalVariableNameContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitWhenExpression(MySQLParser::WhenExpressionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitThenExpression(MySQLParser::ThenExpressionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitElseExpression(MySQLParser::ElseExpressionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitCastType(MySQLParser::CastTypeContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitExprList(MySQLParser::ExprListContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitCharset(MySQLParser::CharsetContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitNotRule(MySQLParser::NotRuleContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitNot2Rule(MySQLParser::Not2RuleContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitInterval(MySQLParser::IntervalContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitIntervalTimeStamp(MySQLParser::IntervalTimeStampContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitExprListWithParentheses(MySQLParser::ExprListWithParenthesesContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitExprWithParentheses(MySQLParser::ExprWithParenthesesContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSimpleExprWithParentheses(MySQLParser::SimpleExprWithParenthesesContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitOrderList(MySQLParser::OrderListContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitOrderExpression(MySQLParser::OrderExpressionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitGroupList(MySQLParser::GroupListContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitGroupingExpression(MySQLParser::GroupingExpressionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitChannel(MySQLParser::ChannelContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitCompoundStatement(MySQLParser::CompoundStatementContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitReturnStatement(MySQLParser::ReturnStatementContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitIfStatement(MySQLParser::IfStatementContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitIfBody(MySQLParser::IfBodyContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitThenStatement(MySQLParser::ThenStatementContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitCompoundStatementList(MySQLParser::CompoundStatementListContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitCaseStatement(MySQLParser::CaseStatementContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitElseStatement(MySQLParser::ElseStatementContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitLabeledBlock(MySQLParser::LabeledBlockContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitUnlabeledBlock(MySQLParser::UnlabeledBlockContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitLabel(MySQLParser::LabelContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitBeginEndBlock(MySQLParser::BeginEndBlockContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitLabeledControl(MySQLParser::LabeledControlContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitUnlabeledControl(MySQLParser::UnlabeledControlContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitLoopBlock(MySQLParser::LoopBlockContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitWhileDoBlock(MySQLParser::WhileDoBlockContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitRepeatUntilBlock(MySQLParser::RepeatUntilBlockContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSpDeclarations(MySQLParser::SpDeclarationsContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSpDeclaration(MySQLParser::SpDeclarationContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitVariableDeclaration(MySQLParser::VariableDeclarationContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitConditionDeclaration(MySQLParser::ConditionDeclarationContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSpCondition(MySQLParser::SpConditionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSqlstate(MySQLParser::SqlstateContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitHandlerDeclaration(MySQLParser::HandlerDeclarationContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitHandlerCondition(MySQLParser::HandlerConditionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitCursorDeclaration(MySQLParser::CursorDeclarationContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitIterateStatement(MySQLParser::IterateStatementContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitLeaveStatement(MySQLParser::LeaveStatementContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitGetDiagnostics(MySQLParser::GetDiagnosticsContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSignalAllowedExpr(MySQLParser::SignalAllowedExprContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitStatementInformationItem(MySQLParser::StatementInformationItemContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitConditionInformationItem(MySQLParser::ConditionInformationItemContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSignalInformationItemName(MySQLParser::SignalInformationItemNameContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSignalStatement(MySQLParser::SignalStatementContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitResignalStatement(MySQLParser::ResignalStatementContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSignalInformationItem(MySQLParser::SignalInformationItemContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitCursorOpen(MySQLParser::CursorOpenContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitCursorClose(MySQLParser::CursorCloseContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitCursorFetch(MySQLParser::CursorFetchContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSchedule(MySQLParser::ScheduleContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitColumnDefinition(MySQLParser::ColumnDefinitionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitCheckOrReferences(MySQLParser::CheckOrReferencesContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitCheckConstraint(MySQLParser::CheckConstraintContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitConstraintEnforcement(MySQLParser::ConstraintEnforcementContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitTableConstraintDef(MySQLParser::TableConstraintDefContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitConstraintName(MySQLParser::ConstraintNameContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitFieldDefinition(MySQLParser::FieldDefinitionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitColumnAttribute(MySQLParser::ColumnAttributeContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitColumnFormat(MySQLParser::ColumnFormatContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitStorageMedia(MySQLParser::StorageMediaContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitGcolAttribute(MySQLParser::GcolAttributeContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitReferences(MySQLParser::ReferencesContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitDeleteOption(MySQLParser::DeleteOptionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitKeyList(MySQLParser::KeyListContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitKeyPart(MySQLParser::KeyPartContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitKeyListWithExpression(MySQLParser::KeyListWithExpressionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitKeyPartOrExpression(MySQLParser::KeyPartOrExpressionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitKeyListVariants(MySQLParser::KeyListVariantsContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitIndexType(MySQLParser::IndexTypeContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitIndexOption(MySQLParser::IndexOptionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitCommonIndexOption(MySQLParser::CommonIndexOptionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitVisibility(MySQLParser::VisibilityContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitIndexTypeClause(MySQLParser::IndexTypeClauseContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitFulltextIndexOption(MySQLParser::FulltextIndexOptionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSpatialIndexOption(MySQLParser::SpatialIndexOptionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitDataTypeDefinition(MySQLParser::DataTypeDefinitionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitDataType(MySQLParser::DataTypeContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitNchar(MySQLParser::NcharContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitRealType(MySQLParser::RealTypeContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitFieldLength(MySQLParser::FieldLengthContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitFieldOptions(MySQLParser::FieldOptionsContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitCharsetWithOptBinary(MySQLParser::CharsetWithOptBinaryContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitAscii(MySQLParser::AsciiContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitUnicode(MySQLParser::UnicodeContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitWsNumCodepoints(MySQLParser::WsNumCodepointsContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitTypeDatetimePrecision(MySQLParser::TypeDatetimePrecisionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitCharsetName(MySQLParser::CharsetNameContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitCollationName(MySQLParser::CollationNameContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitCreateTableOptions(MySQLParser::CreateTableOptionsContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitCreateTableOptionsSpaceSeparated(MySQLParser::CreateTableOptionsSpaceSeparatedContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitCreateTableOption(MySQLParser::CreateTableOptionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitTernaryOption(MySQLParser::TernaryOptionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitDefaultCollation(MySQLParser::DefaultCollationContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitDefaultEncryption(MySQLParser::DefaultEncryptionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitDefaultCharset(MySQLParser::DefaultCharsetContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitPartitionClause(MySQLParser::PartitionClauseContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitPartitionDefKey(MySQLParser::PartitionDefKeyContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitPartitionDefHash(MySQLParser::PartitionDefHashContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitPartitionDefRangeList(MySQLParser::PartitionDefRangeListContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSubPartitions(MySQLParser::SubPartitionsContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitPartitionKeyAlgorithm(MySQLParser::PartitionKeyAlgorithmContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitPartitionDefinitions(MySQLParser::PartitionDefinitionsContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitPartitionDefinition(MySQLParser::PartitionDefinitionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitPartitionValuesIn(MySQLParser::PartitionValuesInContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitPartitionOption(MySQLParser::PartitionOptionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSubpartitionDefinition(MySQLParser::SubpartitionDefinitionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitPartitionValueItemListParen(MySQLParser::PartitionValueItemListParenContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitPartitionValueItem(MySQLParser::PartitionValueItemContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitDefinerClause(MySQLParser::DefinerClauseContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitIfExists(MySQLParser::IfExistsContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitIfNotExists(MySQLParser::IfNotExistsContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitProcedureParameter(MySQLParser::ProcedureParameterContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitFunctionParameter(MySQLParser::FunctionParameterContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitCollate(MySQLParser::CollateContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitTypeWithOptCollate(MySQLParser::TypeWithOptCollateContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSchemaIdentifierPair(MySQLParser::SchemaIdentifierPairContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitViewRefList(MySQLParser::ViewRefListContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitUpdateList(MySQLParser::UpdateListContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitUpdateElement(MySQLParser::UpdateElementContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitCharsetClause(MySQLParser::CharsetClauseContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitFieldsClause(MySQLParser::FieldsClauseContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitFieldTerm(MySQLParser::FieldTermContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitLinesClause(MySQLParser::LinesClauseContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitLineTerm(MySQLParser::LineTermContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitUserList(MySQLParser::UserListContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitCreateUserList(MySQLParser::CreateUserListContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitAlterUserList(MySQLParser::AlterUserListContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitCreateUserEntry(MySQLParser::CreateUserEntryContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitAlterUserEntry(MySQLParser::AlterUserEntryContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitRetainCurrentPassword(MySQLParser::RetainCurrentPasswordContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitDiscardOldPassword(MySQLParser::DiscardOldPasswordContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitReplacePassword(MySQLParser::ReplacePasswordContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitUserIdentifierOrText(MySQLParser::UserIdentifierOrTextContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitUser(MySQLParser::UserContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitLikeClause(MySQLParser::LikeClauseContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitLikeOrWhere(MySQLParser::LikeOrWhereContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitOnlineOption(MySQLParser::OnlineOptionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitNoWriteToBinLog(MySQLParser::NoWriteToBinLogContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitUsePartition(MySQLParser::UsePartitionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitFieldIdentifier(MySQLParser::FieldIdentifierContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitColumnName(MySQLParser::ColumnNameContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitColumnInternalRef(MySQLParser::ColumnInternalRefContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitColumnInternalRefList(MySQLParser::ColumnInternalRefListContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitColumnRef(MySQLParser::ColumnRefContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitInsertIdentifier(MySQLParser::InsertIdentifierContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitIndexName(MySQLParser::IndexNameContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitIndexRef(MySQLParser::IndexRefContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitTableWild(MySQLParser::TableWildContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSchemaName(MySQLParser::SchemaNameContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSchemaRef(MySQLParser::SchemaRefContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitProcedureName(MySQLParser::ProcedureNameContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitProcedureRef(MySQLParser::ProcedureRefContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitFunctionName(MySQLParser::FunctionNameContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitFunctionRef(MySQLParser::FunctionRefContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitTriggerName(MySQLParser::TriggerNameContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitTriggerRef(MySQLParser::TriggerRefContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitViewName(MySQLParser::ViewNameContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitViewRef(MySQLParser::ViewRefContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitTablespaceName(MySQLParser::TablespaceNameContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitTablespaceRef(MySQLParser::TablespaceRefContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitLogfileGroupName(MySQLParser::LogfileGroupNameContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitLogfileGroupRef(MySQLParser::LogfileGroupRefContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitEventName(MySQLParser::EventNameContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitEventRef(MySQLParser::EventRefContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitUdfName(MySQLParser::UdfNameContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitServerName(MySQLParser::ServerNameContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitServerRef(MySQLParser::ServerRefContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitEngineRef(MySQLParser::EngineRefContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitTableName(MySQLParser::TableNameContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitFilterTableRef(MySQLParser::FilterTableRefContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitTableRefWithWildcard(MySQLParser::TableRefWithWildcardContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitTableRef(MySQLParser::TableRefContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitTableRefList(MySQLParser::TableRefListContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitTableAliasRefList(MySQLParser::TableAliasRefListContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitParameterName(MySQLParser::ParameterNameContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitLabelIdentifier(MySQLParser::LabelIdentifierContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitLabelRef(MySQLParser::LabelRefContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitRoleIdentifier(MySQLParser::RoleIdentifierContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitRoleRef(MySQLParser::RoleRefContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitPluginRef(MySQLParser::PluginRefContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitComponentRef(MySQLParser::ComponentRefContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitResourceGroupRef(MySQLParser::ResourceGroupRefContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitWindowName(MySQLParser::WindowNameContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitPureIdentifier(MySQLParser::PureIdentifierContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitIdentifier(MySQLParser::IdentifierContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitIdentifierList(MySQLParser::IdentifierListContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitIdentifierListWithParentheses(MySQLParser::IdentifierListWithParenthesesContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitQualifiedIdentifier(MySQLParser::QualifiedIdentifierContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSimpleIdentifier(MySQLParser::SimpleIdentifierContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitDotIdentifier(MySQLParser::DotIdentifierContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitUlong_number(MySQLParser::Ulong_numberContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitReal_ulong_number(MySQLParser::Real_ulong_numberContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitUlonglong_number(MySQLParser::Ulonglong_numberContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitReal_ulonglong_number(MySQLParser::Real_ulonglong_numberContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitLiteral(MySQLParser::LiteralContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSignedLiteral(MySQLParser::SignedLiteralContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitStringList(MySQLParser::StringListContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitTextStringLiteral(MySQLParser::TextStringLiteralContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitTextString(MySQLParser::TextStringContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitTextStringHash(MySQLParser::TextStringHashContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitTextLiteral(MySQLParser::TextLiteralContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitTextStringNoLinebreak(MySQLParser::TextStringNoLinebreakContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitTextStringLiteralList(MySQLParser::TextStringLiteralListContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitNumLiteral(MySQLParser::NumLiteralContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitBoolLiteral(MySQLParser::BoolLiteralContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitNullLiteral(MySQLParser::NullLiteralContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitTemporalLiteral(MySQLParser::TemporalLiteralContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitFloatOptions(MySQLParser::FloatOptionsContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitStandardFloatOptions(MySQLParser::StandardFloatOptionsContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitPrecision(MySQLParser::PrecisionContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitTextOrIdentifier(MySQLParser::TextOrIdentifierContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitLValueIdentifier(MySQLParser::LValueIdentifierContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitRoleIdentifierOrText(MySQLParser::RoleIdentifierOrTextContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSizeNumber(MySQLParser::SizeNumberContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitParentheses(MySQLParser::ParenthesesContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitEqual(MySQLParser::EqualContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitOptionType(MySQLParser::OptionTypeContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitVarIdentType(MySQLParser::VarIdentTypeContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitSetVarIdentType(MySQLParser::SetVarIdentTypeContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitIdentifierKeyword(MySQLParser::IdentifierKeywordContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitIdentifierKeywordsAmbiguous1RolesAndLabels(MySQLParser::IdentifierKeywordsAmbiguous1RolesAndLabelsContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitIdentifierKeywordsAmbiguous2Labels(MySQLParser::IdentifierKeywordsAmbiguous2LabelsContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitLabelKeyword(MySQLParser::LabelKeywordContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitIdentifierKeywordsAmbiguous3Roles(MySQLParser::IdentifierKeywordsAmbiguous3RolesContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitIdentifierKeywordsUnambiguous(MySQLParser::IdentifierKeywordsUnambiguousContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitRoleKeyword(MySQLParser::RoleKeywordContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitLValueKeyword(MySQLParser::LValueKeywordContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitIdentifierKeywordsAmbiguous4SystemVariables(MySQLParser::IdentifierKeywordsAmbiguous4SystemVariablesContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitRoleOrIdentifierKeyword(MySQLParser::RoleOrIdentifierKeywordContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }

  virtual std::any visitRoleOrLabelKeyword(MySQLParser::RoleOrLabelKeywordContext *ctx) override {
    unsigned int cur_node_hash = hash_array[ctx->getRuleIndex()];
    unsigned int term_token_hash = 0;
    set <unsigned int> seen_token_set;
    for (antlr4::tree::ParseTree* cur_child: ctx->children) {
        if (antlr4::ParserRuleContext* tmp = dynamic_cast<antlr4::ParserRuleContext*>(cur_child)) {
           unsigned int child_rule_hash = hash_array[tmp->getRuleIndex()];
           this->gram_cov.log_edge_cov_map(cur_node_hash, child_rule_hash); 
#ifdef DEBUG
            if (this->gram_cov.has_new_grammar_bits() == 2) {
                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
                cerr << "Child rule: " << p_parser->getRuleNames()[tmp->getRuleIndex()] << "\n";
                cerr << "branch hash: " << ((cur_node_hash >> 1) ^ child_rule_hash) << "\n";
                cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
//            } else {
//                cerr << "Parent and child rule edge seen before. \n\n\n";
//                cerr << "Parent and child rule edge seen before. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
            }
#endif
        } else {
            antlr4::tree::TerminalNode* tmp_token = dynamic_cast<antlr4::tree::TerminalNode*>(cur_child);
            unsigned int cur_token_idx = tmp_token->getSymbol()->getType();
            if (seen_token_set.count(cur_token_idx) != 0) {
#ifdef DEBUG
//                cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//                cerr << "Child token type (repeated, not saved): " << cur_token_idx << "\n\n\n";
#endif 
                continue;
            }
#ifdef DEBUG
//            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
//            cerr << "Child token type: " << cur_token_idx << "\n\n\n";
#endif
            seen_token_set.insert(cur_token_idx);
            cur_token_idx += 1000; // Avoid collision with parser rule's index.
            if (cur_token_idx < 2000) {
                term_token_hash = ((term_token_hash >> 1) ^ hash_array[cur_token_idx]);
            }
        }
    }
    
    if (term_token_hash != 0) {
        this->gram_cov.log_edge_cov_map(cur_node_hash, term_token_hash); 
#ifdef DEBUG
        if (this->gram_cov.has_new_grammar_bits() == 2) {
            cerr << "Current parent rule: " << p_parser->getRuleNames()[ctx->getRuleIndex()] << "\n";
            cerr << "Getting token sequence: ";
            for (auto it = seen_token_set.begin(); it != seen_token_set.end(); it++ ) {
                cerr << " " << *it;
            }
            cerr << "\n";
            cerr << "Child token hash: " << term_token_hash << "\n";
            cerr << "branch hash: " << ((cur_node_hash >> 1) ^ term_token_hash) << "\n";
            cerr << "Triggers new grammar bits. " << this->gram_cov.get_total_edge_cov_size_num() << "\n\n\n";
        }
#endif 
    }
    
    // Iterate to the child node.
    visitChildren(ctx);
    
    return 0;
    
  }


};

#endif
